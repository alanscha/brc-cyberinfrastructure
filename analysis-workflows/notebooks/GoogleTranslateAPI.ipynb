{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Translate API\n",
    "\n",
    "This notebook demonstrates reading from a text file, sending it to the Google translate API, receiving the translation, and writing the output to file. (Nicolas Chan, First created 11/16/2017)\n",
    "\n",
    "### Kernal Setup\n",
    "```bash\n",
    "conda create --name=translate python=3.6 ipykernel\n",
    "source activate translate\n",
    "ipython kernel install --user --name translate\n",
    "pip install --upgrade google-cloud-translate nltk google-api-python-client\n",
    "```\n",
    "### Credentials\n",
    "IMPORTANT: Store your service account JSON credentials in `client_secret.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "target_language = 'en'\n",
    "\n",
    "# Limit maximum character length of text sent to Google translate at once\n",
    "# Larger limit yields better translations because Google translate uses context\n",
    "# Too large of a length might be rejected by Google translate (maximum allowed: 5000)\n",
    "# See: https://cloud.google.com/translate/faq\n",
    "max_length = 4000\n",
    "\n",
    "# Delay between sending chunks\n",
    "delay = 20 # seconds\n",
    "\n",
    "# Set to True only if it makes sense to send chunks split by line breaks.\n",
    "# If line breaks might occur in the middle of sentences, set this to False.\n",
    "preserve_line_breaks = True\n",
    "\n",
    "input_folder = '1dIVXCpexYecYUWj4NdL-4IhGzDMvOakN'\n",
    "completed_folder = '1b9yvvDsm2lH6bLT8wRJnErFSdcMnfwYa'\n",
    "results_folder = '1lVJoATSlYFyb6I_N8EwGVyhSn_AM6vuX'\n",
    "\n",
    "# Credentials\n",
    "translate_secret = 'shapreau_translate.json'\n",
    "google_drive_secret = 'client_secret.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Drive Authentication\n",
    "Google drive interaction uses code from `AdamAndersonFindSumerian.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive authentication based on AdamAndersonFindSumerian.ipynb\n",
    "import codecs\n",
    "import httplib2\n",
    "import os\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from apiclient import discovery, errors\n",
    "from oauth2client import client\n",
    "from oauth2client import tools\n",
    "from oauth2client.file import Storage\n",
    "\n",
    "SCOPES = 'https://www.googleapis.com/auth/drive'\n",
    "CLIENT_SECRET_FILE = 'client_secret.json'\n",
    "APPLICATION_NAME = 'gDriveConnect'\n",
    "\n",
    "def get_credentials():\n",
    "    home_dir = os.path.expanduser('~')\n",
    "    credential_dir = os.path.join(home_dir, '.credentials')\n",
    "    if not os.path.exists(credential_dir):\n",
    "        os.makedirs(credential_dir)\n",
    "    credential_path = os.path.join(credential_dir, 'gDriveConnect.json')\n",
    "    store = Storage(credential_path)    \n",
    "    credentials = store.get()\n",
    "    if not credentials or credentials.invalid:\n",
    "        flow = client.flow_from_clientsecrets(CLIENT_SECRET_FILE, SCOPES)\n",
    "        flow.user_agent = APPLICATION_NAME\n",
    "        if flags:\n",
    "            credentials = tools.run_flow(flow, store, flags)\n",
    "        else: # Needed only for compatibility with Python 2.6\n",
    "            credentials = tools.run(flow, store)\n",
    "        print('Storing credentials to ' + credential_path)\n",
    "    return credentials\n",
    "\n",
    "def get_folder_contents(folder_id):\n",
    "    return service.files().list(\n",
    "        q=\"'\" + folder_id + \"' in parents and trashed=false\", \n",
    "        spaces='drive',\n",
    "        fields='nextPageToken, files(id, name)',\n",
    "        pageToken=None).execute()\n",
    "\n",
    "credentials = get_credentials()\n",
    "print('credential_path:', credentials)\n",
    "http = credentials.authorize(httplib2.Http())\n",
    "service = discovery.build('drive', 'v3', http=http)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify file to process\n",
    "response = get_folder_contents(input_folder)\n",
    "files = list(filter(lambda file: file['name'][-4:] == '.txt', response['files']))\n",
    "if (len(files)):\n",
    "    file = files[0]\n",
    "else:\n",
    "    import sys\n",
    "    sys.exit()\n",
    "    \n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download file\n",
    "# Source: https://developers.google.com/drive/v3/web/manage-downloads\n",
    "# Source: AdamAndersonFindSumerian.ipynb\n",
    "import io, sys\n",
    "def download_file(google_id, destination):\n",
    "    \"\"\"Downloads a file from Google Drive\"\"\"\n",
    "    request = service.files().get_media(fileId=google_id)\n",
    "    fh = io.BytesIO()\n",
    "    downloader = MediaIoBaseDownload(fh, request)\n",
    "    done = False\n",
    "    while not done:\n",
    "        try:\n",
    "            status, done = downloader.next_chunk()\n",
    "            sys.stdout.write('.')\n",
    "        except errors.HttpError as error :\n",
    "            print('Error file:', value, '   id:', key)\n",
    "            print('An error occurred pulling the next chunk:', error)\n",
    "            break\n",
    "    fh.seek(0)\n",
    "    contents = fh.getvalue()\n",
    "    with open(destination, 'wb') as f2:\n",
    "        f2.write(contents)\n",
    "        f2.close()\n",
    "    fh.close()\n",
    "    return contents\n",
    "\n",
    "input_filename = file['id'] + '.txt'\n",
    "output_filename = file['id'] + '_translated.txt'\n",
    "download_file(file['id'], input_filename)\n",
    "\n",
    "# Set tokenizer language\n",
    "# punkt_tokenizer is used to identify where sentences end for splitting into chunks.\n",
    "# It should be set to the INPUT language.\n",
    "# For more info: http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt\n",
    "# Available languages: czech, dutch, estonian, french, greek, norwegian, portuguese, \n",
    "#   spanish, turkish, danish, english, finnish, german, italian, polish, slovene, swedish\n",
    "\n",
    "language_code = file['name'][:3]\n",
    "def lookup_language(code):\n",
    "    languages = {\n",
    "        'cze': 'czech',\n",
    "        'dut': 'dutch',\n",
    "        'est': 'estonian',\n",
    "        'fre': 'french',\n",
    "        'gre': 'greek',\n",
    "        'nor': 'norwegian',\n",
    "        'por': 'portuguese',\n",
    "        'spa': 'spanish',\n",
    "        'tur': 'turkish',\n",
    "        'dan': 'danish',\n",
    "        'eng': 'english',\n",
    "        'fin': 'finnish',\n",
    "        'ger': 'german',\n",
    "        'ita': 'italian',\n",
    "        'pol': 'polish',\n",
    "        'slv': 'slovene',\n",
    "        'swe': 'swedish'\n",
    "    }\n",
    "    return languages[code]\n",
    "punkt_tokenizer = 'tokenizers/punkt/' + lookup_language(language_code) + '.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input file contents\n",
    "input_contents = ''\n",
    "with open(input_filename, encoding='latin-1') as f:\n",
    "    for line in f.readlines():\n",
    "        input_contents += line\n",
    "\n",
    "from html import escape\n",
    "input_contents = escape(input_contents)\n",
    "if preserve_line_breaks:\n",
    "    input_contents = input_contents.replace('\\n', '<br>')\n",
    "    input_contents = input_contents.replace('\\r', '<br>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine sentences into larger chunks\n",
    "def condense(lst, length):\n",
    "    \"\"\"Concatenates elements in lst until each element in lst is at most length\"\"\"\n",
    "    if len(lst) == 0:\n",
    "        return lst\n",
    "    \n",
    "    # Split elements at spaces if they exceed length\n",
    "    number_split = 0\n",
    "    new_lst = []\n",
    "    for elem in lst:\n",
    "        if len(elem) > length:\n",
    "            number_split += 1\n",
    "            new_lst.extend(elem.split(' '))\n",
    "        else:\n",
    "            new_lst.append(elem)\n",
    "    lst = new_lst\n",
    "    \n",
    "    if number_split > 0:\n",
    "        print('WARNING! Had to split', number_split, \n",
    "              'sentences because the sentence length exceeded', length, 'characters.')\n",
    "    if max([ len(elem) for elem in lst ]) > length:\n",
    "        raise Exception('A single word exceeded ' + length + ' characters')\n",
    "    \n",
    "    # Now that all elements are guaranteed to be <= length,\n",
    "    # combine them as long as they do not exceed length.\n",
    "    chunks = []\n",
    "    current_chunk = ''\n",
    "    for sentence in lst:\n",
    "        if len(current_chunk) + len(sentence) < length:\n",
    "            current_chunk += ' ' + sentence\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sentence\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate file contents\n",
    "from google.cloud import translate\n",
    "client = translate.Client.from_service_account_json(translate_secret)\n",
    "def google_translate(text):\n",
    "    return client.translate(text, target_language=target_language)['translatedText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split at sentences\n",
    "# Uses nltk to identify sentence breaks (http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Split into sentences\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load(punkt_tokenizer)\n",
    "\n",
    "import time\n",
    "def translate(text):\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    print('Identified', len(sentences), 'sentences')\n",
    "    \n",
    "    chunks = condense(sentences, max_length)\n",
    "    print('Condensed into', len(chunks), 'chunks')\n",
    "    \n",
    "    translated_chunks = []\n",
    "    for chunk in chunks:\n",
    "        translated_chunks.append(google_translate(chunk))\n",
    "        print('Translated chunk')\n",
    "        time.sleep(delay)\n",
    "    \n",
    "    translation = ' '.join(translated_chunks)\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = translate(input_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation contains escaped HTML charcaters such as '&#39;' for an apostrophe.\n",
    "# To fix this, unescape HTML\n",
    "from html import unescape\n",
    "translation_text = unescape(translation)\n",
    "if preserve_line_breaks:\n",
    "    translation_text = translation_text.replace('<br>', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write output to output file\n",
    "output_file = open(output_filename, 'w', encoding='utf8')\n",
    "output_file.write(translation_text)\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @source Google Drive upload based on AdamAndersonFindSumerian.ipynb\n",
    "def upload_txt_file(name, path, destination_folder=None):\n",
    "    \"\"\"Upload a text file to Google Drive\"\"\"\n",
    "    file_metadata = { 'name': name }\n",
    "    if destination_folder:\n",
    "        file_metadata['parents'] = [destination_folder]\n",
    "    media = MediaFileUpload(path, mimetype='text/plain')\n",
    "    file = service.files().create(\n",
    "        body=file_metadata,\n",
    "        media_body=media,\n",
    "        fields='id'\n",
    "    ).execute()\n",
    "    print('Uploaded', name, '; ID:', file.get('id'))\n",
    "    \n",
    "time.sleep(10)\n",
    "upload_txt_file('eng_translated' + file['name'][3:], output_filename, results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move input text file to completed folder so it is not processed again\n",
    "service.files().update(fileId=file['id'],\n",
    "                       addParents=completed_folder,\n",
    "                       removeParents=input_folder).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translate",
   "language": "python",
   "name": "translate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
