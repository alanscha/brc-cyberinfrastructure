{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Translate API\n",
    "\n",
    "This notebook demonstrates reading from a text file, sending it to the Google translate API, receiving the translation, and writing the output to file. (Nicolas Chan, First created 11/16/2017)\n",
    "\n",
    "### Kernal Setup\n",
    "```bash\n",
    "conda create --name=translate python=3.6 ipykernel\n",
    "source activate translate\n",
    "ipython kernel install --user --name translate\n",
    "pip install --upgrade google-cloud-translate nltk\n",
    "```\n",
    "### Credentials\n",
    "IMPORTANT: Store your service account JSON credentials in `client_secret.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "input_filename = 'input.txt'\n",
    "output_filename = 'output.txt'\n",
    "target_language = 'en'\n",
    "\n",
    "# punkt_tokenizer is used to identify where sentences end for splitting into chunks.\n",
    "# It should be set to the INPUT language.\n",
    "# For more info: http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt\n",
    "# Available languages: czech, dutch, estonian, french, greek, norwegian, portuguese, \n",
    "#   spanish, turkish, danish, english, finnish, german, italian, polish, slovene, swedish\n",
    "punkt_tokenizer = 'tokenizers/punkt/german.pickle'\n",
    "\n",
    "# Limit maximum character length of text sent to Google translate at once\n",
    "# Larger limit yields better translations because Google translate uses context\n",
    "# Too large of a length might be rejected by Google translate (2000 seems safe)\n",
    "max_length = 2000\n",
    "\n",
    "# Set to True only if it makes sense to send chunks split by line breaks.\n",
    "# If line breaks might occur in the middle of sentences, set this to False.\n",
    "preserve_line_breaks = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read input file contents\n",
    "input_contents = ''\n",
    "with open(input_filename, encoding='utf8') as file:\n",
    "    for line in file.readlines():\n",
    "        input_contents += line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to combine sentences into larger chunks\n",
    "def condense(lst, length):\n",
    "    \"\"\"Concatenates elements in lst until each element in lst is at most length\"\"\"\n",
    "    if len(lst) == 0:\n",
    "        return lst\n",
    "    \n",
    "    # Split elements at spaces if they exceed length\n",
    "    number_split = 0\n",
    "    new_lst = []\n",
    "    for elem in lst:\n",
    "        if len(elem) > length:\n",
    "            number_split += 1\n",
    "            new_lst.extend(elem.split(' '))\n",
    "        else:\n",
    "            new_lst.append(elem)\n",
    "    lst = new_lst\n",
    "    \n",
    "    if number_split > 0:\n",
    "        print('WARNING! Had to split', number_split, \n",
    "              'sentences because the sentence length exceeded', length, 'characters.')\n",
    "    if max([ len(elem) for elem in lst ]) > length:\n",
    "        raise Exception('A single word exceeded ' + length + ' characters')\n",
    "    \n",
    "    # Now that all elements are guaranteed to be <= length,\n",
    "    # combine them as long as they do not exceed length.\n",
    "    chunks = []\n",
    "    current_chunk = ''\n",
    "    for sentence in lst:\n",
    "        if len(current_chunk) + len(sentence) < length:\n",
    "            current_chunk += ' ' + sentence\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sentence\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Translate file contents\n",
    "from google.cloud import translate\n",
    "client = translate.Client.from_service_account_json('client_secret.json')\n",
    "def google_translate(text):\n",
    "    return client.translate(text, target_language=target_language)['translatedText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split at sentences\n",
    "# Uses nltk to identify sentence breaks (http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Split into sentences\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load(punkt_tokenizer)\n",
    "\n",
    "def translate(text):\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    print('Identified', len(sentences), 'sentences')\n",
    "    \n",
    "    chunks = condense(sentences, max_length)\n",
    "    print('Condensed into', len(chunks), 'chunks')\n",
    "    \n",
    "    translated_chunks = [ google_translate(chunk) for chunk in chunks ]\n",
    "    translation = ' '.join(translated_chunks)\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from html import escape\n",
    "if preserve_line_breaks:\n",
    "    input_contents = input_contents.replace('\\n', '<br>')\n",
    "translation = translate(input_contents)\n",
    "if preserve_line_breaks:\n",
    "    translation = translation.replace('<br>', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Translation contains escaped HTML charcaters such as '&#39;' for an apostrophe.\n",
    "# To fix this, unescape HTML\n",
    "from html import unescape\n",
    "translation_text = unescape(translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Write output to output file\n",
    "output_file = open(output_filename, 'w', encoding='utf8')\n",
    "output_file.write(translation_text)\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translate",
   "language": "python",
   "name": "translate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
