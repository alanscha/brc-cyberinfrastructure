{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Translate API\n",
    "\n",
    "This notebook demonstrates retrieving text file from Google drive, sending it to the Google translate API, receiving the translation, and uploading the result to Google drive. (Nicolas Chan)\n",
    "\n",
    "### Kernel Setup\n",
    "```bash\n",
    "conda create --name=translate python=3.6 ipykernel\n",
    "source activate translate\n",
    "ipython kernel install --user --name translate\n",
    "pip install --upgrade google-cloud-translate nltk google-api-python-client google-auth-httplib2\n",
    "```\n",
    "### Credentials\n",
    "IMPORTANT: Store your service account JSON credentials in `client_secret.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "target_language = 'en'\n",
    "\n",
    "# Limit maximum character length of text sent to Google translate at once\n",
    "# Larger limit yields better translations because Google translate uses context\n",
    "# Too large of a length might be rejected by Google translate (maximum allowed: 5000)\n",
    "# See: https://cloud.google.com/translate/faq\n",
    "max_length = 4000\n",
    "\n",
    "# Delay between sending chunks\n",
    "delay = 10 # seconds\n",
    "\n",
    "# Set to True only if it makes sense to send chunks split by line breaks.\n",
    "# If line breaks might occur in the middle of sentences, set this to False.\n",
    "preserve_line_breaks = True\n",
    "\n",
    "input_folder = '1dIVXCpexYecYUWj4NdL-4IhGzDMvOakN'\n",
    "completed_folder = '1b9yvvDsm2lH6bLT8wRJnErFSdcMnfwYa'\n",
    "results_folder = '1lVJoATSlYFyb6I_N8EwGVyhSn_AM6vuX'\n",
    "\n",
    "# Credentials\n",
    "# translate_secret = 'shapreau_translate.json'\n",
    "translate_secret = 'shapreau_translate.json'\n",
    "google_drive_secret = 'client_secret.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Drive Authentication\n",
    "Google drive interaction uses code from `AdamAndersonFindSumerian.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gdrive import GDrive\n",
    "gdrive = GDrive(google_drive_secret) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify file to process\n",
    "import sys\n",
    "\n",
    "files = gdrive.list_files(input_folder, 'txt')\n",
    "if (files):\n",
    "    file = files[0]\n",
    "else:\n",
    "    print('No files to process')\n",
    "    sys.exit()\n",
    "    \n",
    "print(\"Found file to process:\", file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download file to translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_filename = file['id'] + '.txt'\n",
    "output_filename = file['id'] + '_translated.txt'\n",
    "input_contents = gdrive.download_file(file['id'], input_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Prefix | Language\n",
    "|---------|---------|\n",
    "| cze | czech |\n",
    "| dut | dutch |\n",
    "| est | estonian |\n",
    "| fre | french |\n",
    "| gre | greek |\n",
    "| nor | norwegian |\n",
    "| por | portuguese |\n",
    "| spa | spanish |\n",
    "| tur | turkish |\n",
    "| dan | danish |\n",
    "| eng | english |\n",
    "| fin | finnish |\n",
    "| ger | german |\n",
    "| ita | italian |\n",
    "| pol | polish |\n",
    "| slv | slovene |\n",
    "| swe | swedish |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tokenizer language\n",
    "# punkt_tokenizer is used to identify where sentences end for splitting into chunks.\n",
    "# It should be set to the INPUT language.\n",
    "# For more info: http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt\n",
    "# Available languages: czech, dutch, estonian, french, greek, norwegian, portuguese, \n",
    "#   spanish, turkish, danish, english, finnish, german, italian, polish, slovene, swedish\n",
    "\n",
    "language_code = file['name'][:3]\n",
    "def lookup_language(code):\n",
    "    languages = {\n",
    "        'cze': 'czech',\n",
    "        'dut': 'dutch',\n",
    "        'est': 'estonian',\n",
    "        'fre': 'french',\n",
    "        'gre': 'greek',\n",
    "        'nor': 'norwegian',\n",
    "        'por': 'portuguese',\n",
    "        'spa': 'spanish',\n",
    "        'tur': 'turkish',\n",
    "        'dan': 'danish',\n",
    "        'eng': 'english',\n",
    "        'fin': 'finnish',\n",
    "        'ger': 'german',\n",
    "        'ita': 'italian',\n",
    "        'pol': 'polish',\n",
    "        'slv': 'slovene',\n",
    "        'swe': 'swedish'\n",
    "    }\n",
    "    return languages[code]\n",
    "punkt_tokenizer = 'tokenizers/punkt/' + lookup_language(language_code) + '.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input file contents\n",
    "input_contents = gdrive.read_local_file(input_filename)\n",
    "\n",
    "from html import escape\n",
    "input_contents = escape(input_contents)\n",
    "if preserve_line_breaks:\n",
    "    input_contents = input_contents.replace('\\n', '<br>')\n",
    "    input_contents = input_contents.replace('\\r', '<br>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine sentences into larger chunks\n",
    "def condense(lst, length):\n",
    "    \"\"\"Concatenates elements in lst until each element in lst is at most length\"\"\"\n",
    "    if len(lst) == 0:\n",
    "        return lst\n",
    "    \n",
    "    # Split elements at spaces if they exceed length\n",
    "    number_split = 0\n",
    "    new_lst = []\n",
    "    for elem in lst:\n",
    "        if len(elem) > length:\n",
    "            number_split += 1\n",
    "            new_lst.extend(elem.split(' '))\n",
    "        else:\n",
    "            new_lst.append(elem)\n",
    "    lst = new_lst\n",
    "    \n",
    "    if number_split > 0:\n",
    "        print('WARNING! Had to split', number_split, \n",
    "              'sentences because the sentence length exceeded', length, 'characters.')\n",
    "    if max([ len(elem) for elem in lst ]) > length:\n",
    "        raise Exception('A single word exceeded ' + length + ' characters')\n",
    "    \n",
    "    # Now that all elements are guaranteed to be <= length,\n",
    "    # combine them as long as they do not exceed length.\n",
    "    chunks = []\n",
    "    current_chunk = ''\n",
    "    for sentence in lst:\n",
    "        if len(current_chunk) + len(sentence) < length:\n",
    "            current_chunk += ' ' + sentence\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sentence\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate file contents\n",
    "from google.cloud import translate\n",
    "client = translate.Client.from_service_account_json(translate_secret)\n",
    "def google_translate(text):\n",
    "    return client.translate(text, target_language=target_language)['translatedText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split at sentences\n",
    "# Uses nltk to identify sentence breaks (http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.punkt)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Split into sentences\n",
    "import nltk.data\n",
    "tokenizer = nltk.data.load(punkt_tokenizer)\n",
    "\n",
    "import time\n",
    "def translate(text):\n",
    "    sentences = tokenizer.tokenize(text)\n",
    "    print('Identified', len(sentences), 'sentences')\n",
    "    \n",
    "    chunks = condense(sentences, max_length)\n",
    "    print('Condensed into', len(chunks), 'chunks')\n",
    "    \n",
    "    translated_chunks = []\n",
    "    for chunk in chunks:\n",
    "        translated_chunks.append(google_translate(chunk))\n",
    "        print('Progress:', str(round(len(translated_chunks) / len(chunks) * 100)) + '%')\n",
    "        time.sleep(delay) # Avoid rate limit error\n",
    "    \n",
    "    translation = ' '.join(translated_chunks)\n",
    "    return translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = translate(input_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation contains escaped HTML charcaters such as '&#39;' for an apostrophe.\n",
    "# To fix this, unescape HTML\n",
    "from html import unescape\n",
    "translation_text = translation\n",
    "if preserve_line_breaks:\n",
    "    translation_text = translation_text.replace('<br>', '\\n')\n",
    "translation_text = unescape(translation_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write output to output file\n",
    "output_file = open(output_filename, 'w', encoding='utf8')\n",
    "print('Writing translation to file', output_filename)\n",
    "output_file.write(translation_text)\n",
    "output_file.close()\n",
    "print('Done writing translation to file')\n",
    "!sync # Make sure the file is finished writing to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Uploading to Google Drive')\n",
    "\n",
    "# Uploading sometimes fails, so try up to 10 times, waiting a minute between each try\n",
    "def upload(tries):\n",
    "    if tries > 10:\n",
    "        print('Maximum tries exceeded, giving up on uploading')\n",
    "        return\n",
    "    try:\n",
    "        print('Upload attempt', tries)\n",
    "        gdrive.upload_file('eng_translated' + file['name'][3:], output_filename, results_folder)\n",
    "        print('Uploaded successfully')\n",
    "    except:\n",
    "        print('Error uploading')\n",
    "        if (tries > 10):\n",
    "            print ('Maximum tries exceeded, giving up on uploading')\n",
    "            gdrive.move_file(file['id'], completed_folder, error_folder)\n",
    "        else:\n",
    "            time.sleep(60)\n",
    "            upload(tries + 1)\n",
    "upload(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move input text file to completed folder so it is not processed again\n",
    "gdrive.move_file(file['id'], completed_folder, input_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "translate2",
   "language": "python",
   "name": "translate2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
