{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook is an exemplar which demonstrates transferring zip files between a bDrive folder and Savio scratch to run OCR on images using Tesseract (inside a Singularity container)\n",
    "\n",
    "( tested with boxsdk (2.0.0a2) on python 3.5 kernel)\n",
    "pip install -Iv boxsdk==2.0.0a2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "_This software is available under the terms of the Educational Community License, Version 2.0 (ECL 2.0). This software is Copyright 2016 The Regents of the University of California, Berkeley (\"Berkeley\")._\n",
    "\n",
    "The text of the ECL license is reproduced below.\n",
    "\n",
    "Educational Community License, Version 2.0\n",
    "*************************************\n",
    "Copyright 2016 The Regents of the University of California, Berkeley (\"Berkeley\")\n",
    "\n",
    "Educational Community License, Version 2.0, April 2007\n",
    "\n",
    "The Educational Community License version 2.0 (\"ECL\") consists of the\n",
    "Apache 2.0 license, modified to change the scope of the patent grant in\n",
    "section 3 to be specific to the needs of the education communities using\n",
    "this license. The original Apache 2.0 license can be found at:[http://www.apache.org/licenses/LICENSE-2.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook configuration section\n",
    "Set of target and source directories, script file names and other used as parameters in processing below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "boxProjectFolder = 'aatest'\n",
    "boxResultsFolder = 'aatest'\n",
    "\n",
    "projectname = 'aatest'\n",
    "runFolder = '/global/scratch/mmanning/aatest/'\n",
    "\n",
    "tesseractimage = '/global/scratch/mmanning/tesseract2_3.img'\n",
    "tesseractdatadir = '/opt/tessdata/'\n",
    "pdfnamelist = []\n",
    "\n",
    "scratchDataDirectory = '/global/scratch/mmanning/aatest/data/'\n",
    "tesseractScratchDataDirectory = '/scratch/'\n",
    "\n",
    "SINGULARITYCMD = 'singularity exec -B /global/scratch/mmanning/aatest/:/scratch/  /global/scratch/mmanning/tesseract2_4.img'\n",
    "\n",
    "gsCommandScript = runFolder + 'gsCommandScript.sh'\n",
    "t4CommandScript = runFolder + 't4CommandScript.sh'\n",
    "slurmScript = runFolder + 'slurmscript.sh'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bDrive Authorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import httplib2\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from apiclient import discovery, errors\n",
    "from oauth2client import client\n",
    "from oauth2client import tools\n",
    "from oauth2client.file import Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SCOPES = 'https://www.googleapis.com/auth/drive'\n",
    "CLIENT_SECRET_FILE = 'client_secret.json'\n",
    "APPLICATION_NAME = 'gDriveConnect'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_credentials():\n",
    "    \n",
    "    home_dir = os.path.expanduser('~')\n",
    "    credential_dir = os.path.join(home_dir, '.credentials')\n",
    "    if not os.path.exists(credential_dir):\n",
    "        os.makedirs(credential_dir)\n",
    "    credential_path = os.path.join(credential_dir, 'gDriveConnect.json')\n",
    "    \n",
    "    store = Storage(credential_path)    \n",
    "    credentials = store.get()\n",
    "    \n",
    "    if not credentials or credentials.invalid:\n",
    "        flow = client.flow_from_clientsecrets(CLIENT_SECRET_FILE, SCOPES)\n",
    "        flow.user_agent = APPLICATION_NAME\n",
    "        if flags:\n",
    "            credentials = tools.run_flow(flow, store, flags)\n",
    "        else: # Needed only for compatibility with Python 2.6\n",
    "            credentials = tools.run(flow, store)\n",
    "        print('Storing credentials to ' + credential_path)\n",
    "        \n",
    "    return credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "credentials = get_credentials()\n",
    "print('credential_path:', credentials)\n",
    "http = credentials.authorize(httplib2.Http())\n",
    "service = discovery.build('drive', 'v3', http=http)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loop thru fodlers to get the target dfolder for the download\n",
    "page_token=None\n",
    "response = service.files().list(q=\"mimeType='application/vnd.google-apps.folder'\",\n",
    "                                     spaces='drive',\n",
    "                                     fields='files(id, name)',\n",
    "                                     pageToken=page_token).execute()\n",
    "targetFolderId = ''\n",
    "for file in response.get('files', []):\n",
    "    if file.get('name')== boxProjectFolder:\n",
    "        targetFolderId = file.get('id')\n",
    "print('target folder id:' + targetFolderId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "downloadMap = {}\n",
    "\n",
    "page_token = None\n",
    "while True:\n",
    "    response = service.files().list(q=\"mimeType='application/pdf' and '0B3QqxeoUcqoAaHV4X1hPYmZNUVE' in parents\",\n",
    "                                         spaces='drive',\n",
    "                                         fields='nextPageToken, files(id, name)',\n",
    "                                         pageToken=page_token).execute()\n",
    "    for file in response.get('files', []):\n",
    "        # Process change\n",
    "        print('Found file: %s (%s)' % (file.get('name'), file.get('id')) )\n",
    "        tup = (file.get('name'), file.get('id'))\n",
    "        downloadMap[file.get('id')]=file.get('name')\n",
    "    page_token = response.get('nextPageToken', None)\n",
    "    if page_token is None:\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "for key, value in downloadMap.items():\n",
    "    print('downloading:', value)\n",
    "    request = service.files().get_media(fileId=key)\n",
    "    fh = io.BytesIO()\n",
    "    downloader = MediaIoBaseDownload(fh, request)\n",
    "    done = False\n",
    "    while done is False:\n",
    "        try:\n",
    "            status, done = downloader.next_chunk()\n",
    "            sys.stdout.write('.')\n",
    "        except errors.HttpError as error :\n",
    "            print('Error file:', value, '   id:', key)\n",
    "            print('An error occurred pulling the next chunk:', error)\n",
    "            break\n",
    "\n",
    "    # failed during download?\n",
    "    if done is False:\n",
    "        continue\n",
    "\n",
    "    outfile = scratchDataDirectory + key + '.pdf'\n",
    "    fh.seek(0)\n",
    "\n",
    "    print('\\nwriting:', outfile)\n",
    "    with open( outfile, 'wb',) as f2:\n",
    "        f2.write(fh.getvalue())\n",
    "        f2.close()\n",
    "\n",
    "    fh.close()\n",
    "print( 'downloads completed.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def natural_sort_key(s, _nsre=re.compile('([0-9]+)')):\n",
    "    return [int(text) if text.isdigit() else text.lower()\n",
    "            for text in re.split(_nsre, s)]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__function to return all files in directory tree.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def scantreeForFiles(path):\n",
    "    \"\"\"Recursively yield DirEntry objects for given directory.\"\"\"\n",
    "    for entry in os.scandir(path):\n",
    "        if entry.is_dir(follow_symlinks=False):\n",
    "            yield from scantreeForFiles(entry.path) \n",
    "        else:\n",
    "            yield entry.path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__function to return list of all folders in directory tree.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def scandirForFolders(path, dirlist):\n",
    "    \"\"\"Recursively yield DirEntry objects for given directory.\"\"\"\n",
    "    for entry in os.scandir(path):\n",
    "        if entry.is_dir(follow_symlinks=False):\n",
    "            dirlist.append(entry.path)\n",
    "            scandirForFolders(entry.path, dirlist)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Validate all the task log files produced by ht_helper __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def validateTaskResults(fileroot, totalTasks):\n",
    "    # file root is job-name.jobId.taskNumber.log\n",
    "    \n",
    "    errorList = []\n",
    "    \n",
    "    for i in range(0, totalTasks-1):\n",
    "        fn = fileroot + '.' + str(i)\n",
    "        if os.path.exists(fn):\n",
    "            out = !tail -1 {fn}\n",
    "            retval = out[0]\n",
    "            #print ('return code: ', out[0])\n",
    "        else:\n",
    "            print ('warning: log file not available: ', fn)\n",
    "        \n",
    "        if ( retval != '0' ):\n",
    "            errorList.append(i)\n",
    "            \n",
    "    return errorList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SLURM job script__ normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# batch script\n",
    "batchtemplate = '#!/bin/bash -l  \\n\\\n",
    "# Job name: \\n\\\n",
    "#SBATCH --job-name=' + projectname + '\\n\\\n",
    "# \\n\\\n",
    "# Account: \\n\\\n",
    "#SBATCH --account=ac_scsguest \\n\\\n",
    "# \\n\\\n",
    "# Partition: \\n\\\n",
    "#SBATCH --partition=savio2 \\n\\\n",
    "# \\n\\\n",
    "## Scale by increasing the number of nodes \\n\\\n",
    "#SBATCH --nodes=5  \\n\\\n",
    "## DO NOT change ntasks-per-node setting as T4 also distributes across cores \\n\\\n",
    "#SBATCH --ntasks-per-node=6 \\n\\\n",
    "#SBATCH --qos=savio_normal \\n\\\n",
    "# \\n\\\n",
    "# Wall clock limit: \\n\\\n",
    "#SBATCH --time={} \\n\\\n",
    "# \\n\\\n",
    "## Command(s) to run: \\n\\\n",
    "module load gcc openmpi  \\n\\\n",
    "/global/home/groups/allhands/bin/ht_helper.sh  -t {} -n1 -s1 -vL \\n' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create script to convert all pdf files in working directory to images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import shutil \n",
    "\n",
    "# Ghostscript executable is inside the container.\n",
    "# TEMPLATE: gs -dBATCH -dNOPAUSE -dQUIET -sDEVICE=png16m -sOutputFile=/scratch/test/output/test-%d.png -r300 /scratch/test/germanocr.pdf\n",
    "SINGULARITYCMD = 'singularity exec -B {}:/scratch/ /global/scratch/mmanning/tesseract2_3.img ' \n",
    "GHOSTSCRIPTCMD = 'gs -dBATCH -dNOPAUSE -dQUIET -sDEVICE=png16m -sOutputFile=\\\"{}-%d.png\\\" -r300 \\\"{}\\\" ;  echo $?'\n",
    "\n",
    "os.chdir(scratchDataDirectory)\n",
    "print ('current working directory: ', os.getcwd())\n",
    "\n",
    "scmd = SINGULARITYCMD.format(scratchDataDirectory)\n",
    "\n",
    "# total number of ghostscript tasks\n",
    "gsCommandTotal = 0\n",
    "\n",
    "with open(gsCommandScript, 'w') as f:  \n",
    "\n",
    "    for entry in scantreeForFiles(scratchDataDirectory):\n",
    "        filename, file_extension = os.path.splitext(entry)\n",
    "        if ( entry.endswith('.pdf')):\n",
    "            relativepath1 = entry[len(scratchDataDirectory):]\n",
    "            relativepath2 = filename[len(scratchDataDirectory):]\n",
    "            gcmd = GHOSTSCRIPTCMD.format(tesseractScratchDataDirectory+relativepath2, tesseractScratchDataDirectory+relativepath1 )\n",
    "            f.write(scmd + gcmd + '\\n')\n",
    "            gsCommandTotal += 1\n",
    "    \n",
    "    \n",
    "#set time limit for this batch run\n",
    "outputbatchscript = batchtemplate.format('00:30:00',  gsCommandScript)\n",
    "with open(slurmScript, 'w') as f:  \n",
    "    f.write(outputbatchscript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Execute the task script with ht_helper__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(runFolder)\n",
    "print ('current working directory: ', os.getcwd())\n",
    "\n",
    "out = !sbatch slurmscript.sh   \n",
    "    \n",
    "print ('Execute ghostscript output: ', out ) \n",
    "jobId =  out[0].split()[3]\n",
    "print (jobId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the users queue and the job status by id\n",
    "!squeue -u $username\n",
    "print('--------------------------------')\n",
    "!scontrol show job $jobId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check all task log files for bad exit code__  \n",
    "task numbers align with lines in the task script  \n",
    "check the log file of tasks in the returned array of failures  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "print ('current working directory: ', os.getcwd())\n",
    "\n",
    "fileroot = projectname + '.' + jobId + '.log'\n",
    "tasklist = validateTaskResults(fileroot, gsCommandTotal)\n",
    "print ('these tasks in task script failed: ', tasklist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remove task logs after any errors have been resolved__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " \n",
    "filter = fileroot + '*'\n",
    "print ('filter: ', filter)\n",
    "for f in glob.glob(filter):\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create script to ocr all png files in working directory to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob, os\n",
    "os.chdir(scratchDataDirectory)\n",
    "print ('current working directory: ', os.getcwd())\n",
    "# template: tesseract --tessdata-dir /opt/tessdata /scratch/germanocr_Page_01.png  germanout  -l deu\n",
    "#TCMD = ' sh -c \\'OMP_NUM_THREADS=1 tesseract --tessdata-dir /opt/tessdata \\\"{}\\\" \\\"{}\\\" \\'  -l deu+eng+tur+fra -c tessedit_create_hocr=1;  echo $?'\n",
    "TCMD = ' sh -c \\'OMP_NUM_THREADS=1 tesseract --tessdata-dir /opt/tessdata  -l deu+eng+tur+fra -c tessedit_create_hocr=1 \\\"{}\\\" \\\"{}\\\" \\';  echo $?'\n",
    "\n",
    "#\n",
    "\n",
    "scmd = SINGULARITYCMD.format(scratchDataDirectory)\n",
    "# total number of tesseract tasks\n",
    "t4CommandTotal = 0\n",
    "\n",
    "with open(t4CommandScript, 'w') as f:\n",
    "\n",
    "    for entry in scantreeForFiles(scratchDataDirectory):\n",
    "        if ( entry.endswith('.png')):\n",
    "            filename, file_extension = os.path.splitext(entry)\n",
    "            relativepath1 = entry[len(scratchDataDirectory):]\n",
    "            relativepath2 = filename[len(scratchDataDirectory):]\n",
    "            tcmd = TCMD.format(tesseractScratchDataDirectory+relativepath1, tesseractScratchDataDirectory+relativepath2 )\n",
    "            #print(scmd + tcmd)\n",
    "            f.write(scmd + tcmd + '\\n')\n",
    "            t4CommandTotal += 1\n",
    "    \n",
    "    \n",
    "#set time limit for this batch run\n",
    "outputbatchscript = batchtemplate.format('03:00:00',  t4CommandScript)\n",
    "with open(slurmScript, 'w') as f:  \n",
    "    f.write(outputbatchscript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Execute the task script with ht_helper__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(runFolder)\n",
    "print ('current working directory: ', os.getcwd())\n",
    "\n",
    "out = !sbatch slurmscript.sh   \n",
    "    \n",
    "print ('Execute tesseract4 output: ', out ) \n",
    "jobId =  out[0].split()[3]\n",
    "print (jobId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the users queue and the job status by id\n",
    "!squeue -u $username\n",
    "print('--------------------------------')\n",
    "!scontrol show job $jobId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check all task log files for bad exit code__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "os.chdir(runFolder)\n",
    "print ('current working directory: ', os.getcwd())\n",
    "\n",
    "fileroot = projectname + '.' + jobId + '.log'\n",
    "#tasklist = validateTaskResults(fileroot, 10) first check a small subset\n",
    "tasklist = validateTaskResults(fileroot, t4CommandTotal)\n",
    "print ('these tasks in task script failed: ', tasklist)\n",
    "\n",
    "# Remove task logs\n",
    "#filter = fileroot + '*'\n",
    "#for f in glob.glob(filter):\n",
    "#    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge text files and upload to Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scandir import scandir\n",
    "dirlist = []\n",
    "\n",
    "scandirForFolders(scratchDataDirectory, dirlist)\n",
    "\n",
    "print(\"num dirs: \", len(dirlist) ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__check that for every .png there is a .hocr in each directory__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missingResultList = []\n",
    "for currentdir in dirlist:\n",
    "    os.chdir(currentdir)\n",
    "    #print ('current working directory: ', os.getcwd())\n",
    "    \n",
    "    \n",
    "    # get a list of all pdf names\n",
    "    for filename in os.listdir(os.getcwd()):\n",
    "        if  os.path.isfile(filename)  and filename.endswith('.png'):\n",
    "            fn, fe = os.path.splitext(filename)\n",
    "            if not os.path.exists(fn + '.hocr'):\n",
    "                missingResultList.append(currentdir + '/' + filename)\n",
    "                print ('missing result: ', currentdir + '/' + filename )\n",
    "print(\"missingResultList size: \", len(missingResultList) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "__process hocr files__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current scoring approach:\n",
    "- To be counted, the score for a word must be between 25 and 70, this removes some garbage characters at the low end.\n",
    "- If a line contains between 6 and 9 scoring words it registers as a small hit, if it contains 10 or more then the line registers as a big hit.\n",
    "- If the total score for the last three lines (a rolling window) is > 15 then that is a small hit and > 25 is a big hit.\n",
    "- if a \"paragraph\" (currently using the dev tag in the hocr xml) has > 25 scoring words that is a small hit and > 40 is a bit hit.\n",
    "This scoring approach seems to be doing a good job of finding target text. However, it also includes a number of false positives that I \n",
    "have not been able to reduce significantly. Table and figures are usually tagged as hits. The Teissier_Sealings doc has a number of tables, \n",
    "rotated to landscape, which show up in the big hits list. I have not found a way to identify a table or figure from the xml results. \n",
    "There are a couple papers online with complex detection algorithms but nothing I could implement without significant development time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "srch = re.compile(r'.*[-—]+.*[-—]+[.-—]*')\n",
    "\n",
    "def vetHitList(hitlist):\n",
    "\n",
    "    #logging.info('vetHitList  hitlist: %s ', hitlist )\n",
    "    totalHits= len(hitlist)\n",
    "    count = 0\n",
    "    for hit in hitlist:\n",
    "        found = srch.search(hit)\n",
    "        if found:\n",
    "            #logging.info('good hit: %s ', hit )\n",
    "            count = count + 1\n",
    "        #else:\n",
    "            #logging.info('BAD hit:  %s ', hit )\n",
    "\n",
    "    percent = count / totalHits\n",
    "    #logging.info('vetHitList percent: %s ', percent)\n",
    "    if percent > .666:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "def parseHocrFiles(filenameroot, fileList):\n",
    "\n",
    "    logging.basicConfig(handlers=[logging.FileHandler('ocrparse.log', 'w', 'utf-8')], level=logging.INFO, format='%(message)s',  datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    last_three_lines = collections.deque(3*[0], 3)\n",
    "    last_three_line_words = collections.deque(3*[''], 3)\n",
    "    div_count=0\n",
    "    avg_low_score = 0\n",
    "    low_score_ctr = 0\n",
    "    low_score_words = []\n",
    "    div_words = []\n",
    "    line_id = ''\n",
    "    bighits = []\n",
    "    smallhits = []\n",
    "\n",
    "    for filename in fileList :\n",
    "\n",
    "        if not filename.endswith(\".hocr\"):\n",
    "            continue\n",
    "\n",
    "        print (' filename: ', filename)\n",
    "\n",
    "        # split out the file name and the page (image) number\n",
    "        splittokens = re.split(r\"-|\\.\", filename)\n",
    "        tot = len(splittokens)\n",
    "        image_number = splittokens[tot - 2]\n",
    "        image_number_decimal = int(image_number.strip())\n",
    "        #doc_name = splittokens[0]\n",
    "        print (' doc name: ', filenameroot)\n",
    "        print (' image number: ', image_number)\n",
    "        soup = BeautifulSoup(open(filename, encoding='utf-8'), \"html5lib\")\n",
    "        #print ('==========>', filename )\n",
    "        logging.info(\"==========> %s\", filename )\n",
    "\n",
    "        last_three_lines.clear()\n",
    "        last_three_line_words.clear()\n",
    "\n",
    "        for div_tag in soup.find_all('div'):\n",
    "\n",
    "\n",
    "            div_id = div_tag['id']\n",
    "            if div_id is None:\n",
    "                div_id = 'None'\n",
    "            else:\n",
    "                div_id = div_id.strip()\n",
    "\n",
    "\n",
    "            div_count = len(div_words)\n",
    "            #check words in hit list for hypens\n",
    "            #logging.info(\"div words: %s \", div_words)\n",
    "\n",
    "            if div_count > 0 :\n",
    "                gooddivwords = vetHitList( div_words )\n",
    "            else:\n",
    "                gooddivwords = False\n",
    "\n",
    "            if gooddivwords :\n",
    "                logging.info(\"good div words: %s \", div_words)\n",
    "\n",
    "            # if more than 25 words in this dev section then add to hit list\n",
    "            if div_count > 20 and gooddivwords:\n",
    "                bighits.append([filenameroot, image_number_decimal, \"div count: \" + str(div_count), div_words] )\n",
    "                logging.info(\"file: %s  div count: %d tag: %s \", filename, div_count, div_id )\n",
    "            elif div_count > 10 and gooddivwords:\n",
    "                smallhits.append([filenameroot, image_number_decimal, \"div count: \" + str(div_count), div_words] )\n",
    "                logging.info(\"file: %s  div count: %d tag: %s \", filename, div_count, div_id )\n",
    "\n",
    "            div_count = 0\n",
    "            div_words = []\n",
    "\n",
    "\n",
    "            #print 'tag initial: ', tag\n",
    "            #print ('tag class: ', div_tag['class'] )\n",
    "            if 'ocr_page' in div_tag['class']:\n",
    "                #logging.info(\"ocr_page: %s\" % tag['title'])\n",
    "                #print 'tag filtered: ', tag\n",
    "                for span_tag in div_tag.find_all('span'):\n",
    "                    #print spantag\n",
    "\n",
    "                    if 'ocr_line' in span_tag['class']:\n",
    "                        line_id = span_tag['id'].strip().encode('utf-8')\n",
    "                        #print ('new line :', line_id,  ' process prev set then reset counters')\n",
    "\n",
    "                        #check words in hit list for hypens\n",
    "                        if len(low_score_words) > 0:\n",
    "                            goodwords = vetHitList( low_score_words )\n",
    "                        else:\n",
    "                            goodwords = False\n",
    "                        #print(\"goodwords: \", goodwords)\n",
    "                        lsw = [x.encode('utf-8') for x in low_score_words]\n",
    "                        if low_score_ctr > 6 and low_score_ctr <= 9 and goodwords :\n",
    "                            \n",
    "                            print ('mid range hit: ',  lsw  )\n",
    "                            logging.info(\"line:  %s   score: %d   avg low score: %f  words:  %s\",  line_id, low_score_ctr, (avg_low_score/low_score_ctr) , low_score_words  )\n",
    "                            #smallhits.append( [filenameroot, image_number_decimal, low_score_ctr, low_score_words] )\n",
    "                            smallhits.append( [filenameroot, image_number_decimal, low_score_ctr, lsw] )\n",
    "\n",
    "                        if low_score_ctr >= 10 and goodwords :\n",
    "                            \n",
    "                            print ('high range hit', lsw  )\n",
    "                            logging.info(\"line:  %s   score: %d    avg low score: %f  words:  %s\",  line_id, low_score_ctr, (avg_low_score/low_score_ctr),   low_score_words )\n",
    "                            #bighits.append( [filenameroot, image_number_decimal, low_score_ctr, low_score_words] )\n",
    "                            bighits.append( [filenameroot, image_number_decimal, low_score_ctr, lsw] )\n",
    "\n",
    "                        div_words.extend(low_score_words)\n",
    "\n",
    "\n",
    "                        # add to the counter of the last three lines and if total is over the threahold then log\n",
    "                        last_three_lines.appendleft(low_score_ctr)\n",
    "                        last_three_line_words.appendleft(low_score_words)\n",
    "                        total_last_three_lines = sum(last_three_lines)\n",
    "                        if total_last_three_lines > 25 :\n",
    "                            logging.info(\"line:  %s   last three lines:  %s\",  line_id, last_three_lines )\n",
    "                            bighits.append( [filenameroot, image_number_decimal, \"three line total:\" + str(total_last_three_lines) , list(last_three_line_words) ] )\n",
    "                        elif total_last_three_lines > 15 :\n",
    "                            logging.info(\"line:  %s   last three lines:  %s\",  line_id, last_three_lines )\n",
    "                            smallhits.append( [filenameroot, image_number_decimal, \"three line total:\" + str(total_last_three_lines), list(last_three_line_words) ] )\n",
    "\n",
    "\n",
    "                        low_score_words = []\n",
    "                        avg_low_score = 0\n",
    "                        low_score_ctr = 0\n",
    "\n",
    "                        # that is all the processing when a new line is reached\n",
    "                        continue\n",
    "\n",
    "                    if span_tag.string is None:\n",
    "                        continue\n",
    "\n",
    "                    spantagword = span_tag.string.strip()\n",
    "                    #print ('span tag: ', spantagword.encode(\"utf-8\")  )\n",
    "                    span_title_split = span_tag['title'].split(';')\n",
    "                    for span_title_element in span_title_split:\n",
    "                        if 'x_wconf' in span_title_element:\n",
    "                            #label, score = title_element.split(' ')\n",
    "                            score = span_title_element.replace('x_wconf', '').strip()\n",
    "                            #print( 'word: ', spantagword.encode(\"utf-8\"), 'score: ', int(score.strip()) )\n",
    "\n",
    "                            # if score less than 25 the could be table. diagram, or figure\n",
    "                            if int(score.strip())  < 70 and int(score.strip()) > 25 :\n",
    "                                #logging.info('word:  %s score: %s ',  spantagconverted, score.strip() )\n",
    "                                low_score_ctr = low_score_ctr + 1\n",
    "                                low_score_words.append( spantagword )\n",
    "                                avg_low_score = avg_low_score + int(score.strip())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #files to hold totalsi\n",
    "    print(\"create results files for: \", filenameroot)\n",
    "    bighitssorted = open( scratchDataDirectory + filenameroot + '_bighits.txt', 'w', encoding=\"utf-8\")\n",
    "    smallhitssorted = open( scratchDataDirectory + filenameroot + '_smallhits.txt', 'w', encoding=\"utf-8\")\n",
    "\n",
    "    bigsortedlist =  sorted(bighits, key=lambda row: row[1], reverse=False)\n",
    "    logging.info('bigsortedlist: %s ', bigsortedlist)\n",
    "    smallsortedlist =  sorted(smallhits, key=lambda row: row[1], reverse=False)\n",
    "    logging.info('smallsortedlist: %s ', smallsortedlist)\n",
    "\n",
    "    unique = []\n",
    "    for hit in bigsortedlist:\n",
    "        if hit[1] not in unique :\n",
    "            unique.append( hit[1] )\n",
    "            print(\"big hit:\", hit)\n",
    "            #bighitssorted.write(hit[0] + ';' + hit[1] + ';' + hit[3]  + \"\\n\"  )\n",
    "            thehit = str(hit[3])\n",
    "            #bighitssorted.write( str(hit[3])   )\n",
    "            bighitssorted.write(str(hit[0]) + ';' + str(hit[1]) + ';' + str(hit[3])  + \"\\n\" )\n",
    "    bighitssorted.close()\n",
    "\n",
    "    for hit in smallsortedlist:\n",
    "        if hit[1] not in unique :\n",
    "            unique.append( hit[1] )\n",
    "            print(\"small hit:\", hit)\n",
    "            smallhitssorted.write(str(hit[0]) + ';' + str(hit[1]) + ';' + str(hit[3])  + \"\\n\"  )\n",
    "            #smallhitssorted.write(str(hit[0]).encode('utf-8') + ';' + str(hit[1]).encode('utf-8') + ';' + str(hit[3]).encode('utf-8') + \"\\n\"  )\n",
    "    smallhitssorted.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenameList = []\n",
    "\n",
    "#import fnmatch\n",
    "#for filename in os.listdir(scratchDataDirectory):\n",
    "    #print(\"filename: \", filename)\n",
    "#    if filename.endswith(\".pdf\") :\n",
    "\n",
    "        # split out the file name and the page (image) number\n",
    "#        splittokens = re.split(r\"-|\\.\", filename)\n",
    "#        tot = len(splittokens)\n",
    "#        filenameList.append( splittokens[0] )\n",
    "#print(\"filenameList: \", filenameList )\n",
    "            \n",
    "pattern = '*.hocr'\n",
    "#for filenameroot in filenameList:\n",
    "for key, value in downloadMap.items():\n",
    "    print('processing: ' + value)\n",
    "    hocrfileList = []\n",
    "    for hocrname in os.listdir(scratchDataDirectory):\n",
    "        if (fnmatch.fnmatch(hocrname, pattern) and hocrname.startswith(key) ):\n",
    "            hocrfileList.append(scratchDataDirectory+hocrname)\n",
    "\n",
    "    print(\"\\n\\nparseOcrOutputForFileset list: \", hocrfileList)\n",
    "    parseHocrFiles(filenameroot, hocrfileList)\n",
    "    print(\"parseOcrOutputForFileset completed file set parse: \", filenameroot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__merge all the big hit and small hit result files__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "big_hit_list = runFolder + 'bighitlist.txt'\n",
    "small_hit_list = runFolder + 'smallhitlist.txt'\n",
    "completedSet = set()\n",
    "\n",
    "fbigout = open (big_hit_list, 'a')\n",
    "fsmallout = open (small_hit_list, 'a')\n",
    "\n",
    "\n",
    "smallfiles = glob.glob('*smallhits*')\n",
    "\n",
    "for fs in smallfiles:\n",
    "    hitid, *rest  = fs.split('_')\n",
    "    completedSet.add(hitid)\n",
    "\n",
    "    idname = ''\n",
    "    if hitid in totalMap:\n",
    "       idname = totalMap[hitid]\n",
    "       print(' ******name: ', idname)\n",
    "\n",
    "   \n",
    "    for line in open(fs):\n",
    "        print('line1: ', line)\n",
    "        out = line.strip('\\n') + ';' + idname + '\\n'\n",
    "        sys.stdout.write('out1: '+ out)\n",
    "        fsmallout.write(out)\n",
    "    \n",
    "\n",
    "fsmallout.close()\n",
    "\n",
    "\n",
    "bigfiles = glob.glob('*bighits*')\n",
    "\n",
    "for fb in bigfiles:\n",
    "    hitid, *rest  = fb.split('_')\n",
    "    completedSet.add(hitid)\n",
    "\n",
    "    name = ''\n",
    "    if hitid in totalMap:\n",
    "       name = totalMap[hitid]\n",
    "       #print(' ******name: ', idname)\n",
    "\n",
    "   \n",
    "    for line in open(fb):\n",
    "        #print('line2: ', line)\n",
    "        out = line.strip('\\n') + ';' + idname + '\\n'\n",
    "        sys.stdout.write('out2: ' + out)\n",
    "        fbigout.write(out)\n",
    "     \n",
    "\n",
    "fbigout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__cleanup__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"num dirs: \", len(dirlist) ) \n",
    "\n",
    "for currentdir in dirlist:\n",
    "    os.chdir(currentdir)\n",
    "    print ('current working directory: ', os.getcwd())\n",
    "    \n",
    "    # remove all pdf and png files\n",
    "    for currentFile in os.listdir(os.getcwd()):\n",
    "        if os.path.isfile(currentFile) and not currentFile.endswith('hits.txt'):\n",
    "                os.remove(os.path.join(currentdir, currentFile))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Move the resulting zip file to bDrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_metadata = { 'name' : 'hitlist.txt' }\n",
    "media = MediaFileUpload(runFolder + 'hitlist.txt',\n",
    "                        mimetype='text/plain')\n",
    "file = drive_service.files().create(body=file_metadata,\n",
    "                                    media_body=media,\n",
    "                                    fields='id').execute()\n",
    "print 'File ID: %s' % file.get('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
