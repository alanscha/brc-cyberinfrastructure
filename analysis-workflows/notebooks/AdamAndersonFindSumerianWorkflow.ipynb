{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### This notebook is an exemplar which demonstrates transferring zip files between a bDrive folder and Savio scratch to run OCR on images using Tesseract (inside a Singularity container)\n",
    "\n",
    "( tested with boxsdk (2.0.0a2) on python 3.5 kernel)\n",
    "pip install -Iv boxsdk==2.0.0a2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "_This software is available under the terms of the Educational Community License, Version 2.0 (ECL 2.0). This software is Copyright 2016 The Regents of the University of California, Berkeley (\"Berkeley\")._\n",
    "\n",
    "The text of the ECL license is reproduced below.\n",
    "\n",
    "Educational Community License, Version 2.0\n",
    "*************************************\n",
    "Copyright 2017 The Regents of the University of California, Berkeley (\"Berkeley\")\n",
    "\n",
    "Educational Community License, Version 2.0, April 2007\n",
    "\n",
    "The Educational Community License version 2.0 (\"ECL\") consists of the\n",
    "Apache 2.0 license, modified to change the scope of the patent grant in\n",
    "section 3 to be specific to the needs of the education communities using\n",
    "this license. The original Apache 2.0 license can be found at:[http://www.apache.org/licenses/LICENSE-2.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook configuration section\n",
    "Set of target and source directories, script file names and other used as parameters in processing below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "savioUsername = 'nicolaschan' #Put your savio username here\n",
    "boxProjectFolder = 'aatest' #Put the name of your Google Drive Folder with data here, ensure that it is NOT nested\n",
    "boxResultsFolder = 'aatest' #Put the name of the Drive Folder where you would like results placed. \n",
    "projectname = 'aanderson'         #Put the name of the folder in your scratch folder you would like data stored in\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Added by Nicolas Chan, 10/12/2017\n",
    "number_to_process = 2\n",
    "shared_results_folder_id = '0B1297pLT9WXLV29iNmlWcTlvakk'\n",
    "all_files_list = '/global/scratch/groups/dh/aanderson/all_files.txt'\n",
    "group_results_folder = '/global/scratch/groups/dh/aanderson/results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "runFolder = '/global/scratch/' + savioUsername + '/' + projectname + '/'\n",
    "\n",
    "tesseractimage = '/global/scratch/groups/dh/tesseract2_3.img'\n",
    "tesseractdatadir = '/opt/tessdata/'\n",
    "pdfnamelist = []\n",
    "\n",
    "scratchDataDirectory = '/global/scratch/' + savioUsername + '/' + projectname + '/data/'\n",
    "tesseractScratchDataDirectory = '/scratch/'\n",
    "\n",
    "SINGULARITYCMD = 'singularity exec -B ' + runFolder + ':/scratch/  ' + tesseractimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make Directories if they do not exist\n",
    "import os\n",
    "import sys\n",
    "import errno\n",
    "\n",
    "def dir_create(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exception:\n",
    "        if exception.errno != errno.EEXIST:\n",
    "            raise\n",
    "        else:\n",
    "            print('Folder at: ' + path + ' already exists. Skipping...')\n",
    "# dir_create(runFolder)\n",
    "# dir_create(scratchDataDirectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bDrive Authorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import httplib2\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "from apiclient import discovery, errors\n",
    "from oauth2client import client\n",
    "from oauth2client import tools\n",
    "from oauth2client.file import Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SCOPES = 'https://www.googleapis.com/auth/drive'\n",
    "CLIENT_SECRET_FILE = 'client_secret.json'\n",
    "APPLICATION_NAME = 'gDriveConnect'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser(parents=[tools.argparser])\n",
    "parser.add_argument('-f', help=argparse.SUPPRESS)\n",
    "\n",
    "flags = parser.parse_known_args()[0]\n",
    "flags.noauth_local_webserver = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_credentials():\n",
    "    \n",
    "    home_dir = os.path.expanduser('~')\n",
    "    credential_dir = os.path.join(home_dir, '.credentials')\n",
    "    if not os.path.exists(credential_dir):\n",
    "        os.makedirs(credential_dir)\n",
    "    credential_path = os.path.join(credential_dir, 'gDriveConnect.json')\n",
    "    \n",
    "    store = Storage(credential_path)    \n",
    "    credentials = store.get()\n",
    "    \n",
    "    if not credentials or credentials.invalid:\n",
    "        flow = client.flow_from_clientsecrets(CLIENT_SECRET_FILE, SCOPES)\n",
    "        flow.user_agent = APPLICATION_NAME\n",
    "        if flags:\n",
    "            credentials = tools.run_flow(flow, store, flags)\n",
    "        else: # Needed only for compatibility with Python 2.6\n",
    "            credentials = tools.run(flow, store)\n",
    "        print('Storing credentials to ' + credential_path)\n",
    "        \n",
    "    return credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "credential_path: <oauth2client.client.OAuth2Credentials object at 0x2b76e9ebff28>\n"
     ]
    }
   ],
   "source": [
    "credentials = get_credentials()\n",
    "print('credential_path:', credentials)\n",
    "http = credentials.authorize(httplib2.Http())\n",
    "service = discovery.build('drive', 'v3', http=http)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target folder id:0B1297pLT9WXLdjdGTV9FbHE5NXc\n"
     ]
    }
   ],
   "source": [
    "# loop thru folders to get the target dfolder for the download\n",
    "page_token=None\n",
    "response = service.files().list(q=\"mimeType='application/vnd.google-apps.folder'\",\n",
    "                                     spaces='drive',\n",
    "                                     fields='files(id, name)',\n",
    "                                     pageToken=page_token).execute()\n",
    "targetFolderId = ''\n",
    "for file in response.get('files', []):\n",
    "    if file.get('name')== boxProjectFolder:\n",
    "        targetFolderId = file.get('id')\n",
    "print('target folder id:' + targetFolderId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Results_145-146', 'Results_147-151', 'Results_143-144', 'Results_0-140', 'Results_141-142']\n",
      "Will process range 152 to 153\n"
     ]
    }
   ],
   "source": [
    "# Added by Nicolas Chan, 10/12/2017\n",
    "# Figure out where to start in the range\n",
    "\n",
    "# Get names of all the folders\n",
    "def get_drive_contents(folder_id):\n",
    "    contents = []\n",
    "    query=\"'\" + folder_id + \"' in parents and trashed=false\"\n",
    "    \n",
    "    # This implementation is copied from below (commented out)\n",
    "    page_token = None\n",
    "    while True:\n",
    "        response = service.files().list(q=query,\n",
    "                                             spaces='drive',\n",
    "                                             fields='nextPageToken, files(id, name)',\n",
    "                                             pageToken=page_token).execute()\n",
    "        for file in response.get('files', []):\n",
    "            # Process change\n",
    "            print('Found file: %s (%s)' % (file.get('name'), file.get('id')) )\n",
    "            tup = (file.get('name'), file.get('id'))\n",
    "            contents.append(tup)\n",
    "        page_token = response.get('nextPageToken', None)\n",
    "        if page_token is None:\n",
    "            break;\n",
    "    return contents\n",
    "\n",
    "def results_folder_to_range(folder_name):\n",
    "    start, end = folder_name.split('_')[1].split('-')\n",
    "    return int(start), int(end)\n",
    "\n",
    "# results_folders = get_drive_contents(shared_results_folder_id)\n",
    "results_folders = [ d for d in os.listdir(group_results_folder) ]\n",
    "print(os.listdir(group_results_folder))\n",
    "\n",
    "completed_up_to = -1\n",
    "for folder in results_folders:\n",
    "    folder_name = folder\n",
    "    _, end = results_folder_to_range(folder_name)\n",
    "    completed_up_to = max(completed_up_to, end)\n",
    "\n",
    "completed_up_to += 1 # Start on next one\n",
    "#henry:should be number_to_process + completed_up_to - 1\n",
    "#start_index, end_index = completed_up_to, number_to_process + start_index - 1\n",
    "start_index, end_index = completed_up_to, number_to_process + completed_up_to - 1\n",
    "print('Will process range', start_index, 'to', end_index)\n",
    "log = \"Processing from index {0} to index {1}\".format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Nicolas Chan, 10/17/2017\n",
    "# Create the directory for this run inside of the projectname folder\n",
    "runFolder = runFolder + 'range_' + str(start_index) + '-' + str(end_index) + '/'\n",
    "scratchDataDirectory = runFolder + 'data/'\n",
    "\n",
    "gsCommandScript = runFolder + 'gsCommandScript.sh'\n",
    "t4CommandScript = runFolder + 't4CommandScript.sh'\n",
    "slurmScript = runFolder + 'slurmscript.sh'\n",
    "\n",
    "dir_create(runFolder)\n",
    "dir_create(scratchDataDirectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-19 14:55:15 Start processing index 152 to index 153\n"
     ]
    }
   ],
   "source": [
    "#Henry Ang 10/13/2017\n",
    "from time import localtime, strftime\n",
    "logMsg = \"{2} Start processing index {0} to index {1}\".format(start_index, end_index, strftime(\"%Y-%m-%d %H:%M:%S\", localtime())\n",
    ")\n",
    "print(logMsg)\n",
    "!echo $logMsg >> /global/scratch/groups/dh/aanderson/process_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/global/scratch/groups/dh/aanderson/results//Results_152-153/\n"
     ]
    }
   ],
   "source": [
    "# Added by Nicolas Chan, 10/12/2017\n",
    "def create_drive_folder(name, parent=None):\n",
    "    \"Create a new Google Drive folder, nested under parent\"\n",
    "    \n",
    "    body = { 'name': name, 'mimeType': 'application/vnd.google-apps.folder' }\n",
    "    if parent:\n",
    "        body['parents'] = [parent]\n",
    "    return service.files().create(body = body).execute()\n",
    "\n",
    "# Create a results folder for this range\n",
    "results_folder_name = 'Results_' + str(start_index) + '-' + str(end_index)\n",
    "# results_folder = create_drive_folder(results_folder_name, shared_results_folder_id)\n",
    "results_folder = group_results_folder + '/' + results_folder_name + '/'\n",
    "dir_create(results_folder)\n",
    "print(results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Added by Nicolas Chan based on previous code, 10/12/2017\n",
    "import io\n",
    "def download_file(google_id, destination):\n",
    "    \"\"\"Downloads a file from Google Drive\"\"\"\n",
    "    \n",
    "    request = service.files().get_media(fileId=google_id)\n",
    "    fh = io.BytesIO()\n",
    "    downloader = MediaIoBaseDownload(fh, request)\n",
    "    \n",
    "    done = False\n",
    "    while done is False:\n",
    "        try:\n",
    "            status, done = downloader.next_chunk()\n",
    "            sys.stdout.write('.')\n",
    "        except errors.HttpError as error :\n",
    "            print('Error file:', value, '   id:', key)\n",
    "            print('An error occurred pulling the next chunk:', error)\n",
    "            break\n",
    "\n",
    "    fh.seek(0)\n",
    "\n",
    "    print('\\nwriting:', destination)\n",
    "    with open( destination, 'wb',) as f2:\n",
    "        f2.write(fh.getvalue())\n",
    "        f2.close()\n",
    "\n",
    "    fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 files to download\n"
     ]
    }
   ],
   "source": [
    "# Added by Nicolas Chan, 10/12/2017\n",
    "# Find which files to use in the range specified\n",
    "files_to_download = []\n",
    "with open(all_files_list, 'rt', encoding='utf-8') as f:\n",
    "    line_number = 0\n",
    "    for line in f:\n",
    "        if line_number > end_index:\n",
    "            break\n",
    "        if line_number >= start_index:\n",
    "            files_to_download.append(line.split(';')[-1].strip('\\n'))\n",
    "        line_number += 1\n",
    "\n",
    "print(len(files_to_download), 'files to download')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-19 14:55:18 Start downloading PDFs\n"
     ]
    }
   ],
   "source": [
    "#Henry Ang 10/13/2017\n",
    "logMsg = \"{0} Start downloading PDFs\".format(strftime(\"%Y-%m-%d %H:%M:%S\", localtime()))\n",
    "print(logMsg)\n",
    "!echo $logMsg >> /global/scratch/groups/dh/aanderson/process_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............\n",
      "writing: /global/scratch/nicolaschan/aanderson/range_152-153/data/0B9Ibqa26YXiRUFRISE9EdmZjUW8.pdf\n",
      "........................\n",
      "writing: /global/scratch/nicolaschan/aanderson/range_152-153/data/0B9Ibqa26YXiRYVlrZ0xuck85OW8.pdf\n"
     ]
    }
   ],
   "source": [
    "# Added by Nicolas Chan, 10/12/2017\n",
    "# Download specified files\n",
    "for file in files_to_download:\n",
    "    download_file(file, scratchDataDirectory + file + '.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def natural_sort_key(s, _nsre=re.compile('([0-9]+)')):\n",
    "    return [int(text) if text.isdigit() else text.lower()\n",
    "            for text in re.split(_nsre, s)]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__function to return all files in directory tree.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def scantreeForFiles(path):\n",
    "    \"\"\"Recursively yield DirEntry objects for given directory.\"\"\"\n",
    "    for entry in os.scandir(path):\n",
    "        if entry.is_dir(follow_symlinks=False):\n",
    "            yield from scantreeForFiles(entry.path) \n",
    "        else:\n",
    "            yield entry.path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__function to return list of all folders in directory tree.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def scandirForFolders(path, dirlist):\n",
    "    \"\"\"Recursively yield DirEntry objects for given directory.\"\"\"\n",
    "    for entry in os.scandir(path):\n",
    "        if entry.is_dir(follow_symlinks=False):\n",
    "            dirlist.append(entry.path)\n",
    "            scandirForFolders(entry.path, dirlist)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Validate all the task log files produced by ht_helper __"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def validateTaskResults(fileroot, totalTasks):\n",
    "    # file root is job-name.jobId.taskNumber.log\n",
    "    \n",
    "    errorList = []\n",
    "    \n",
    "    for i in range(0, totalTasks-1):\n",
    "        fn = fileroot + '.' + str(i)\n",
    "        if os.path.exists(fn):\n",
    "            out = !tail -1 {fn}\n",
    "            retval = out[0]\n",
    "            #print ('return code: ', out[0])\n",
    "        else:\n",
    "            print ('warning: log file not available: ', fn)\n",
    "        \n",
    "        if ( retval != '0' ):\n",
    "            errorList.append(i)\n",
    "            \n",
    "    return errorList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__SLURM job script__ normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# batch script\n",
    "batchtemplate = '#!/bin/bash -l  \\n\\\n",
    "# Job name: \\n\\\n",
    "#SBATCH --job-name=' + projectname + '\\n\\\n",
    "# \\n\\\n",
    "# Account: \\n\\\n",
    "#SBATCH --account=ac_scsguest \\n\\\n",
    "# \\n\\\n",
    "# Partition: \\n\\\n",
    "#SBATCH --partition=savio2 \\n\\\n",
    "# \\n\\\n",
    "## Scale by increasing the number of nodes \\n\\\n",
    "#SBATCH --nodes=5  \\n\\\n",
    "## DO NOT change ntasks-per-node setting as T4 also distributes across cores \\n\\\n",
    "#SBATCH --ntasks-per-node=6 \\n\\\n",
    "#SBATCH --qos=savio_normal \\n\\\n",
    "# \\n\\\n",
    "# Wall clock limit: \\n\\\n",
    "#SBATCH --time={} \\n\\\n",
    "# \\n\\\n",
    "## Command(s) to run: \\n\\\n",
    "module load gcc openmpi  \\n\\\n",
    "/global/home/groups/allhands/bin/ht_helper.sh  -t {} -n1 -s1 -vL \\n' \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create script to convert all pdf files in working directory to images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working directory:  /global/scratch/nicolaschan/aanderson/range_152-153/data\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import shutil \n",
    "\n",
    "# Ghostscript executable is inside the container.\n",
    "# TEMPLATE: gs -dBATCH -dNOPAUSE -dQUIET -sDEVICE=png16m -sOutputFile=/scratch/test/output/test-%d.png -r300 /scratch/test/germanocr.pdf\n",
    "SINGULARITYCMD = 'singularity exec -B {}:/scratch/ /global/scratch/groups/dh/tesseract2_3.img ' \n",
    "GHOSTSCRIPTCMD = 'gs -dBATCH -dNOPAUSE -dQUIET -sDEVICE=png16m -sOutputFile=\\\"{}-%d.png\\\" -r300 \\\"{}\\\" ;  echo $?'\n",
    "\n",
    "os.chdir(scratchDataDirectory)\n",
    "print ('current working directory: ', os.getcwd())\n",
    "\n",
    "scmd = SINGULARITYCMD.format(scratchDataDirectory)\n",
    "\n",
    "# total number of ghostscript tasks\n",
    "gsCommandTotal = 0\n",
    "\n",
    "with open(gsCommandScript, 'w') as f:  \n",
    "\n",
    "    for entry in scantreeForFiles(scratchDataDirectory):\n",
    "        filename, file_extension = os.path.splitext(entry)\n",
    "        if ( entry.endswith('.pdf')):\n",
    "            relativepath1 = entry[len(scratchDataDirectory):]\n",
    "            relativepath2 = filename[len(scratchDataDirectory):]\n",
    "            gcmd = GHOSTSCRIPTCMD.format(tesseractScratchDataDirectory+relativepath2, tesseractScratchDataDirectory+relativepath1 )\n",
    "            f.write(scmd + gcmd + '\\n')\n",
    "            gsCommandTotal += 1\n",
    "    \n",
    "    \n",
    "#set time limit for this batch run\n",
    "outputbatchscript = batchtemplate.format('00:30:00',  gsCommandScript)\n",
    "with open(slurmScript, 'w') as f:  \n",
    "    f.write(outputbatchscript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Execute the task script with ht_helper__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working directory:  /global/scratch/nicolaschan/aanderson/range_152-153\n",
      "Execute ghostscript output:  ['Submitted batch job 1836808']\n",
      "1836808\n"
     ]
    }
   ],
   "source": [
    "os.chdir(runFolder)\n",
    "print ('current working directory: ', os.getcwd())\n",
    "\n",
    "out = !sbatch slurmscript.sh   \n",
    "    \n",
    "print ('Execute ghostscript output: ', out ) \n",
    "jobId =  out[0].split()[3]\n",
    "print (jobId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-19 15:10:12 Start converting PDF to PNG, job ID:1836808\n"
     ]
    }
   ],
   "source": [
    "#Henry Ang 10/13/2017\n",
    "logMsg = \"{1} Start converting PDF to PNG, job ID:{0}\".format(jobId, strftime(\"%Y-%m-%d %H:%M:%S\", localtime()))\n",
    "print(logMsg)\n",
    "!echo $logMsg >> /global/scratch/groups/dh/aanderson/process_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "           1836808    savio2 aanderso nicolasc PD       0:00      5 (None)\n",
      "--------------------------------\n",
      "Savio Job has been submitted. This cell will notify you when the job is done.\n",
      "........\n",
      "******Savio Job finished******\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# print the users queue and the job status by id\n",
    "!squeue -u $savioUsername #possibly do not need\n",
    "print('--------------------------------')\n",
    "print('Savio Job has been submitted. This cell will notify you when the job is done.')\n",
    "jobState = False\n",
    "while not jobState:\n",
    "    out = !scontrol show job $jobId\n",
    "    if any(\"COMPLETED\" in s for s in out):\n",
    "        print('\\n******Savio Job finished******')\n",
    "        jobState = True\n",
    "    else:\n",
    "       print('.', end='')\n",
    "       time.sleep(10) #Can tweak this so that people can see it moving\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check all task log files for bad exit code__  \n",
    "task numbers align with lines in the task script  \n",
    "check the log file of tasks in the returned array of failures  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working directory:  /global/scratch/nicolaschan/aanderson/range_152-153\n",
      "these tasks in task script failed:  [0]\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "if (not jobState):\n",
    "    print(\"WARNING: Your SLURM Job has not finished processing! Please wait for the cell above to complete.\")\n",
    "else:\n",
    "    print ('current working directory: ', os.getcwd())\n",
    "    fileroot = projectname + '.' + jobId + '.log'\n",
    "    tasklist = validateTaskResults(fileroot, gsCommandTotal)\n",
    "    print ('these tasks in task script failed: ', tasklist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remove task logs after any errors have been resolved__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filter:  aanderson.1836808.log*\n"
     ]
    }
   ],
   "source": [
    " \n",
    "filter = fileroot + '*'\n",
    "print ('filter: ', filter)\n",
    "for f in glob.glob(filter):\n",
    "    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create script to ocr all png files in working directory to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working directory:  /global/scratch/nicolaschan/aanderson/range_152-153/data\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "os.chdir(scratchDataDirectory)\n",
    "print ('current working directory: ', os.getcwd())\n",
    "# template: tesseract --tessdata-dir /opt/tessdata /scratch/germanocr_Page_01.png  germanout  -l deu\n",
    "#TCMD = ' sh -c \\'OMP_NUM_THREADS=1 tesseract --tessdata-dir /opt/tessdata \\\"{}\\\" \\\"{}\\\" \\'  -l deu+eng+tur+fra -c tessedit_create_hocr=1;  echo $?'\n",
    "TCMD = ' sh -c \\'OMP_NUM_THREADS=1 tesseract --tessdata-dir /opt/tessdata  -l deu+eng+tur+fra -c tessedit_create_hocr=1 \\\"{}\\\" \\\"{}\\\" \\';  echo $?'\n",
    "\n",
    "#\n",
    "\n",
    "scmd = SINGULARITYCMD.format(scratchDataDirectory)\n",
    "# total number of tesseract tasks\n",
    "t4CommandTotal = 0\n",
    "\n",
    "with open(t4CommandScript, 'w') as f:\n",
    "\n",
    "    for entry in scantreeForFiles(scratchDataDirectory):\n",
    "        if ( entry.endswith('.png')):\n",
    "            filename, file_extension = os.path.splitext(entry)\n",
    "            relativepath1 = entry[len(scratchDataDirectory):]\n",
    "            relativepath2 = filename[len(scratchDataDirectory):]\n",
    "            tcmd = TCMD.format(tesseractScratchDataDirectory+relativepath1, tesseractScratchDataDirectory+relativepath2 )\n",
    "            #print(scmd + tcmd)\n",
    "            f.write(scmd + tcmd + '\\n')\n",
    "            t4CommandTotal += 1\n",
    "    \n",
    "    \n",
    "#set time limit for this batch run\n",
    "outputbatchscript = batchtemplate.format('03:00:00',  t4CommandScript)\n",
    "with open(slurmScript, 'w') as f:  \n",
    "    f.write(outputbatchscript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Execute the task script with ht_helper__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working directory:  /global/scratch/nicolaschan/aanderson/range_152-153\n",
      "Execute tesseract4 output:  ['Submitted batch job 1836809']\n",
      "1836809\n"
     ]
    }
   ],
   "source": [
    "os.chdir(runFolder)\n",
    "print ('current working directory: ', os.getcwd())\n",
    "\n",
    "out = !sbatch slurmscript.sh   \n",
    "    \n",
    "print ('Execute tesseract4 output: ', out ) \n",
    "jobId =  out[0].split()[3]\n",
    "print (jobId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-19 15:11:35 Start converting PNG to text, job ID:1836809\n"
     ]
    }
   ],
   "source": [
    "#Henry Ang 10/13/2017\n",
    "logMsg = \"{1} Start converting PNG to text, job ID:{0}\".format(jobId, strftime(\"%Y-%m-%d %H:%M:%S\", localtime()))\n",
    "print(logMsg)\n",
    "!echo $logMsg >> /global/scratch/groups/dh/aanderson/process_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "           1836809    savio2 aanderso nicolasc PD       0:00      5 (None)\n",
      "--------------------------------\n",
      "Savio Job has been submitted. This cell will notify you when the job is done.\n",
      "................\n",
      "******Savio Job finished******\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# print the users queue and the job status by id\n",
    "!squeue -u $savioUsername #possibly do not need\n",
    "print('--------------------------------')\n",
    "print('Savio Job has been submitted. This cell will notify you when the job is done.')\n",
    "jobState = False\n",
    "while not jobState:\n",
    "    out = !scontrol show job $jobId\n",
    "    if any(\"COMPLETED\" in s for s in out):\n",
    "        print('\\n******Savio Job finished******')\n",
    "        jobState = True\n",
    "    else:\n",
    "       print('.', end='')\n",
    "       time.sleep(10) #Can tweak this so that people can see it moving\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check all task log files for bad exit code__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working directory:  /global/scratch/nicolaschan/aanderson/range_152-153\n",
      "these tasks in task script failed:  []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.chdir(runFolder)\n",
    "print ('current working directory: ', os.getcwd())\n",
    "\n",
    "fileroot = projectname + '.' + jobId + '.log'\n",
    "#tasklist = validateTaskResults(fileroot, 10) first check a small subset\n",
    "tasklist = validateTaskResults(fileroot, t4CommandTotal)\n",
    "print ('these tasks in task script failed: ', tasklist)\n",
    "\n",
    "# Remove task logs\n",
    "#filter = fileroot + '*'\n",
    "#for f in glob.glob(filter):\n",
    "#    os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge text files and upload to Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num dirs:  0\n"
     ]
    }
   ],
   "source": [
    "from scandir import scandir\n",
    "dirlist = []\n",
    "\n",
    "scandirForFolders(scratchDataDirectory, dirlist)\n",
    "\n",
    "print(\"num dirs: \", len(dirlist) ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__check that for every .png there is a .hocr in each directory__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missingResultList size:  0\n"
     ]
    }
   ],
   "source": [
    "missingResultList = []\n",
    "for currentdir in dirlist:\n",
    "    os.chdir(currentdir)\n",
    "    #print ('current working directory: ', os.getcwd())\n",
    "    \n",
    "    \n",
    "    # get a list of all pdf names\n",
    "    for filename in os.listdir(os.getcwd()):\n",
    "        if  os.path.isfile(filename)  and filename.endswith('.png'):\n",
    "            fn, fe = os.path.splitext(filename)\n",
    "            if not os.path.exists(fn + '.hocr'):\n",
    "                missingResultList.append(currentdir + '/' + filename)\n",
    "                print ('missing result: ', currentdir + '/' + filename )\n",
    "print(\"missingResultList size: \", len(missingResultList) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "__process hocr files__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current scoring approach:\n",
    "- To be counted, the score for a word must be between 25 and 70, this removes some garbage characters at the low end.\n",
    "- If a line contains between 6 and 9 scoring words it registers as a small hit, if it contains 10 or more then the line registers as a big hit.\n",
    "- If the total score for the last three lines (a rolling window) is > 15 then that is a small hit and > 25 is a big hit.\n",
    "- if a \"paragraph\" (currently using the dev tag in the hocr xml) has > 25 scoring words that is a small hit and > 40 is a bit hit.\n",
    "This scoring approach seems to be doing a good job of finding target text. However, it also includes a number of false positives that I \n",
    "have not been able to reduce significantly. Table and figures are usually tagged as hits. The Teissier_Sealings doc has a number of tables, \n",
    "rotated to landscape, which show up in the big hits list. I have not found a way to identify a table or figure from the xml results. \n",
    "There are a couple papers online with complex detection algorithms but nothing I could implement without significant development time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "srch = re.compile(r'.*[-—]+.*[-—]+[.-—]*')\n",
    "\n",
    "def vetHitList(hitlist):\n",
    "\n",
    "    #logging.info('vetHitList  hitlist: %s ', hitlist )\n",
    "    totalHits= len(hitlist)\n",
    "    count = 0\n",
    "    for hit in hitlist:\n",
    "        found = srch.search(hit)\n",
    "        if found:\n",
    "            #logging.info('good hit: %s ', hit )\n",
    "            count = count + 1\n",
    "        #else:\n",
    "            #logging.info('BAD hit:  %s ', hit )\n",
    "\n",
    "    percent = count / totalHits\n",
    "    #logging.info('vetHitList percent: %s ', percent)\n",
    "    if percent > .666:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "import lxml\n",
    "import lxml.etree\n",
    "import bs4.builder._lxml\n",
    "import bs4.builder._html5lib\n",
    "\n",
    "\n",
    "\n",
    "def parseHocrFiles(filenameroot, fileList):\n",
    "\n",
    "    logging.basicConfig(handlers=[logging.FileHandler('ocrparse.log', 'w', 'utf-8')], level=logging.INFO, format='%(message)s',  datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    last_three_lines = collections.deque(3*[0], 3)\n",
    "    last_three_line_words = collections.deque(3*[''], 3)\n",
    "    div_count=0\n",
    "    avg_low_score = 0\n",
    "    low_score_ctr = 0\n",
    "    low_score_words = []\n",
    "    div_words = []\n",
    "    line_id = ''\n",
    "    bighits = []\n",
    "    smallhits = []\n",
    "\n",
    "    for filename in fileList :\n",
    "\n",
    "        if not filename.endswith(\".hocr\"):\n",
    "            continue\n",
    "\n",
    "        #print (' filename: ', filename)\n",
    "\n",
    "        # split out the file name and the page (image) number\n",
    "        splittokens = re.split(r\"-|\\.\", filename)\n",
    "        tot = len(splittokens)\n",
    "        image_number = splittokens[tot - 2]\n",
    "        image_number_decimal = int(image_number.strip())\n",
    "        #doc_name = splittokens[0]\n",
    "        #print (' doc name: ', filenameroot)\n",
    "        #print (' image number: ', image_number)\n",
    "        soup = BeautifulSoup(open(filename, encoding='utf-8'), \"lxml\")\n",
    "        #print ('==========>', filename )\n",
    "        logging.info(\"==========> %s\", filename )\n",
    "\n",
    "        last_three_lines.clear()\n",
    "        last_three_line_words.clear()\n",
    "\n",
    "        for div_tag in soup.find_all('div'):\n",
    "\n",
    "\n",
    "            div_id = div_tag['id']\n",
    "            if div_id is None:\n",
    "                div_id = 'None'\n",
    "            else:\n",
    "                div_id = div_id.strip()\n",
    "\n",
    "\n",
    "            div_count = len(div_words)\n",
    "            #check words in hit list for hypens\n",
    "            #logging.info(\"div words: %s \", div_words)\n",
    "\n",
    "            if div_count > 0 :\n",
    "                gooddivwords = vetHitList( div_words )\n",
    "            else:\n",
    "                gooddivwords = False\n",
    "\n",
    "            if gooddivwords :\n",
    "                logging.info(\"good div words: %s \", div_words)\n",
    "\n",
    "            # if more than 25 words in this dev section then add to hit list\n",
    "            if div_count > 20 and gooddivwords:\n",
    "                bighits.append([filenameroot, image_number_decimal, \"div count: \" + str(div_count), div_words] )\n",
    "                logging.info(\"file: %s  div count: %d tag: %s \", filename, div_count, div_id )\n",
    "            elif div_count > 10 and gooddivwords:\n",
    "                smallhits.append([filenameroot, image_number_decimal, \"div count: \" + str(div_count), div_words] )\n",
    "                logging.info(\"file: %s  div count: %d tag: %s \", filename, div_count, div_id )\n",
    "\n",
    "            div_count = 0\n",
    "            div_words = []\n",
    "\n",
    "\n",
    "            #print 'tag initial: ', tag\n",
    "            #print ('tag class: ', div_tag['class'] )\n",
    "            if 'ocr_page' in div_tag['class']:\n",
    "                #logging.info(\"ocr_page: %s\" % tag['title'])\n",
    "                #print 'tag filtered: ', tag\n",
    "                for span_tag in div_tag.find_all('span'):\n",
    "                    #print spantag\n",
    "\n",
    "                    if 'ocr_line' in span_tag['class']:\n",
    "                        line_id = span_tag['id'].strip().encode('utf-8')\n",
    "                        #print ('new line :', line_id,  ' process prev set then reset counters')\n",
    "\n",
    "                        #check words in hit list for hypens\n",
    "                        if len(low_score_words) > 0:\n",
    "                            goodwords = vetHitList( low_score_words )\n",
    "                        else:\n",
    "                            goodwords = False\n",
    "                        #print(\"goodwords: \", goodwords)\n",
    "                        lsw = [x.encode('utf-8') for x in low_score_words]\n",
    "                        if low_score_ctr > 6 and low_score_ctr <= 9 and goodwords :\n",
    "                            \n",
    "                            print ('mid range hit: ',  lsw  )\n",
    "                            logging.info(\"line:  %s   score: %d   avg low score: %f  words:  %s\",  line_id, low_score_ctr, (avg_low_score/low_score_ctr) , low_score_words  )\n",
    "                            #smallhits.append( [filenameroot, image_number_decimal, low_score_ctr, low_score_words] )\n",
    "                            smallhits.append( [filenameroot, image_number_decimal, low_score_ctr, lsw] )\n",
    "\n",
    "                        if low_score_ctr >= 10 and goodwords :\n",
    "                            \n",
    "                            print ('high range hit', lsw  )\n",
    "                            logging.info(\"line:  %s   score: %d    avg low score: %f  words:  %s\",  line_id, low_score_ctr, (avg_low_score/low_score_ctr),   low_score_words )\n",
    "                            #bighits.append( [filenameroot, image_number_decimal, low_score_ctr, low_score_words] )\n",
    "                            bighits.append( [filenameroot, image_number_decimal, low_score_ctr, lsw] )\n",
    "\n",
    "                        div_words.extend(low_score_words)\n",
    "\n",
    "\n",
    "                        # add to the counter of the last three lines and if total is over the threahold then log\n",
    "                        last_three_lines.appendleft(low_score_ctr)\n",
    "                        last_three_line_words.appendleft(low_score_words)\n",
    "                        total_last_three_lines = sum(last_three_lines)\n",
    "                        if total_last_three_lines > 25 :\n",
    "                            logging.info(\"line:  %s   last three lines:  %s\",  line_id, last_three_lines )\n",
    "                            bighits.append( [filenameroot, image_number_decimal, \"three line total:\" + str(total_last_three_lines) , list(last_three_line_words) ] )\n",
    "                        elif total_last_three_lines > 15 :\n",
    "                            logging.info(\"line:  %s   last three lines:  %s\",  line_id, last_three_lines )\n",
    "                            smallhits.append( [filenameroot, image_number_decimal, \"three line total:\" + str(total_last_three_lines), list(last_three_line_words) ] )\n",
    "\n",
    "\n",
    "                        low_score_words = []\n",
    "                        avg_low_score = 0\n",
    "                        low_score_ctr = 0\n",
    "\n",
    "                        # that is all the processing when a new line is reached\n",
    "                        continue\n",
    "\n",
    "                    if span_tag.string is None:\n",
    "                        continue\n",
    "\n",
    "                    spantagword = span_tag.string.strip()\n",
    "                    #print ('span tag: ', spantagword.encode(\"utf-8\")  )\n",
    "                    span_title_split = span_tag['title'].split(';')\n",
    "                    for span_title_element in span_title_split:\n",
    "                        if 'x_wconf' in span_title_element:\n",
    "                            #label, score = title_element.split(' ')\n",
    "                            score = span_title_element.replace('x_wconf', '').strip()\n",
    "                            #print( 'word: ', spantagword.encode(\"utf-8\"), 'score: ', int(score.strip()) )\n",
    "\n",
    "                            # if score less than 25 the could be table. diagram, or figure\n",
    "                            if int(score.strip())  < 70 and int(score.strip()) > 25 :\n",
    "                                #logging.info('word:  %s score: %s ',  spantagconverted, score.strip() )\n",
    "                                low_score_ctr = low_score_ctr + 1\n",
    "                                low_score_words.append( spantagword )\n",
    "                                avg_low_score = avg_low_score + int(score.strip())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #files to hold totalsi\n",
    "    #print(\"create results files for: \", filenameroot)\n",
    "    bighitssorted = open( scratchDataDirectory + filenameroot + '_bighits.txt', 'w', encoding=\"utf-8\")\n",
    "    smallhitssorted = open( scratchDataDirectory + filenameroot + '_smallhits.txt', 'w', encoding=\"utf-8\")\n",
    "\n",
    "    bigsortedlist =  sorted(bighits, key=lambda row: row[1], reverse=False)\n",
    "    logging.info('bigsortedlist: %s ', bigsortedlist)\n",
    "    smallsortedlist =  sorted(smallhits, key=lambda row: row[1], reverse=False)\n",
    "    logging.info('smallsortedlist: %s ', smallsortedlist)\n",
    "\n",
    "    unique = []\n",
    "    for hit in bigsortedlist:\n",
    "        if hit[1] not in unique :\n",
    "            unique.append( hit[1] )\n",
    "            print(\"big hit:\", hit)\n",
    "            #bighitssorted.write(hit[0] + ';' + hit[1] + ';' + hit[3]  + \"\\n\"  )\n",
    "            thehit = str(hit[3])\n",
    "            #bighitssorted.write( str(hit[3])   )\n",
    "            bighitssorted.write(str(hit[0]) + ';' + str(hit[1]) + ';' + str(hit[3])  + \"\\n\" )\n",
    "    bighitssorted.close()\n",
    "\n",
    "    for hit in smallsortedlist:\n",
    "        if hit[1] not in unique :\n",
    "            unique.append( hit[1] )\n",
    "            print(\"small hit:\", hit)\n",
    "            smallhitssorted.write(str(hit[0]) + ';' + str(hit[1]) + ';' + str(hit[3])  + \"\\n\"  )\n",
    "            #smallhitssorted.write(str(hit[0]).encode('utf-8') + ';' + str(hit[1]).encode('utf-8') + ';' + str(hit[3]).encode('utf-8') + \"\\n\"  )\n",
    "    smallhitssorted.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "small hit: ['0B9Ibqa26YXiRUFRISE9EdmZjUW8', 8, 'div count: 13', ['(da-mim)', 'A-wa-anki).23', 'A-wa-anki-ka.28', '(dlnğuğinak)', \"be-rlz'1-[su]\", 'Al-wa-anki', 'Sim-bi-iğ-hu-uk', 'dlnğuğinak', 'ib-ba-al-su-ğum', 'rkil-ib-m-tinfı', \"rcır1-ba-ı'rrı\", 'sVaŸ—x—[x]', '§a?-rx‘-[x])']]\n",
      "big hit: ['0B9Ibqa26YXiRYVlrZ0xuck85OW8', 27, 'three line total:33', [['CX', 'CK', 'CZ', 'Blldsdoliifusion', '“P', 'mier', '°', \"'\", '““”\"“on', '“amg', 'seahngs', '”*', '189’', '197’', '207’', '214’', '213’'], ['iğ', 'ü', 'ï', 'Ing', \"'\", 'Wit', '&'], ['l', '.]', '(liundZîî', 'altsma', 'fumpdotf', 'Chill:', 'wine', 'halîiji', '%']]]\n",
      "big hit: ['0B9Ibqa26YXiRYVlrZ0xuck85OW8', 40, 'three line total:27', [['_î—', '“-', 'J', \"'\", 'N‘»;', '---j_', 'I', \"'l'ıl\"], ['WEI;', 'î', 'Fl', '%%', '@', 'UU', '@', 'EF', 'wt', '%', '.::“?', '515,1', '\"', 'L'], ['we:', \"l'\", '1.15,', ':,', ':']]]\n",
      "small hit: ['0B9Ibqa26YXiRYVlrZ0xuck85OW8', 41, 'three line total:17', [['ll', '43', '171', 'ËÊ', 'Iamğad'], ['11', '„um?', 'ihl\"?', 'ä', '&', '““.:', '_', 'ra', 'Nıq(mepub?'], [',', 'li!»', 'w']]]\n",
      "\n",
      "\n",
      "Finished Processing OCR Files.\n"
     ]
    }
   ],
   "source": [
    "filenameList = []\n",
    "\n",
    "import fnmatch\n",
    "#for filename in os.listdir(scratchDataDirectory):\n",
    "    #print(\"filename: \", filename)\n",
    "#    if filename.endswith(\".pdf\") :\n",
    "\n",
    "        # split out the file name and the page (image) number\n",
    "#        splittokens = re.split(r\"-|\\.\", filename)\n",
    "#        tot = len(splittokens)\n",
    "#        filenameList.append( splittokens[0] )\n",
    "#print(\"filenameList: \", filenameList )\n",
    "            \n",
    "pattern = '*.hocr'\n",
    "#for filenameroot in filenameList:\n",
    "# for key, value in downloadMap.items():\n",
    "for key in files_to_download:\n",
    "    #print('processing: ' + value)\n",
    "    hocrfileList = []\n",
    "    for hocrname in os.listdir(scratchDataDirectory):\n",
    "        if (fnmatch.fnmatch(hocrname, pattern) and hocrname.startswith(key) ):\n",
    "            hocrfileList.append(scratchDataDirectory+hocrname)\n",
    "\n",
    "    #print(\"\\n\\nparseOcrOutputForFileset list: \", hocrfileList)\n",
    "    parseHocrFiles(key, hocrfileList)\n",
    "    #print(\"parseOcrOutputForFileset completed file set parse: \", key)\n",
    "print('\\n\\nFinished Processing OCR Files.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__merge all the big hit and small hit result files__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each document has its own bighits and smallhits files, located in `data/$GOOGLEID_bighits.txt` and `data/$GOOGLEID_smallhits.txt`\n",
    "- Merge these into a `bighitlist.txt` and `smallhitlist.txt` for the whole run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Added by Nicolas Chan, 10/12/2017\n",
    "!cat data/*smallhits* > smallhitlist.txt\n",
    "!cat data/*bighits* > bighitlist.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__cleanup__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num dirs:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"num dirs: \", len(dirlist) ) \n",
    "\n",
    "for currentdir in dirlist:\n",
    "    os.chdir(currentdir)\n",
    "    print ('current working directory: ', os.getcwd())\n",
    "    \n",
    "    # remove all pdf and png files\n",
    "    for currentFile in os.listdir(os.getcwd()):\n",
    "        if os.path.isfile(currentFile) and not currentFile.endswith('hits.txt'):\n",
    "                os.remove(os.path.join(currentdir, currentFile))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Move the resulting zip file to bDrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Added by Nicolas Chan based on previous code, 10/12/2017\n",
    "def upload_txt_file(name, path, destination_folder=None):\n",
    "    \"\"\"Upload a text file to Google Drive\"\"\"\n",
    "    \n",
    "    file_metadata = { 'name': name }\n",
    "    if destination_folder:\n",
    "        file_metadata['parents'] = [destination_folder]\n",
    "    media = MediaFileUpload(path, mimetype='text/plain')\n",
    "    file = service.files().create(\n",
    "        body=file_metadata,\n",
    "        media_body=media,\n",
    "        fields='id'\n",
    "    ).execute()\n",
    "    print('Uploaded', name, '; ID:', file.get('id'))\n",
    "\n",
    "# upload_txt_file('smallhitlist.txt', runFolder + 'smallhitlist.txt', results_folder['id'])\n",
    "# upload_txt_file('bighitlist.txt', runFolder + 'bighitlist.txt', results_folder['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/global/scratch/groups/dh/aanderson/results//Results_152-153//bighitlist.txt'"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copy results to shared folder\n",
    "import shutil\n",
    "shutil.copyfile(runFolder + 'smallhitlist.txt', results_folder + '/smallhitlist.txt')\n",
    "shutil.copyfile(runFolder + 'bighitlist.txt', results_folder + '/bighitlist.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-19 15:14:26 Finished processing from index 152 to index 153\n"
     ]
    }
   ],
   "source": [
    "#Henry Ang 10/13/2017\n",
    "logMsg = \"{2} Finished processing from index {0} to index {1}\".format(start_index, end_index, strftime(\"%Y-%m-%d %H:%M:%S\", localtime())\n",
    ")\n",
    "print(logMsg)\n",
    "!echo $logMsg >> /global/scratch/groups/dh/aanderson/process_log.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocrPython36opencv",
   "language": "python",
   "name": "ocrpython36opencv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
